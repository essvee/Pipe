{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from annette.db import SessionManager\n",
    "from annette.db.models import Citation, ManualClassification\n",
    "import dill\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a generic function to get a list of `{'attribute_name': attribute, 'class': classification}` records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_data(attr_name):\n",
    "    attr = getattr(Citation, attr_name)\n",
    "\n",
    "    with SessionManager() as session_manager:\n",
    "        citations = session_manager.session.query(attr,\n",
    "                                                  ManualClassification.classification_id) \\\n",
    "            .join(ManualClassification, Citation.doi == ManualClassification.doi) \\\n",
    "            .group_by(attr, ManualClassification.classification_id).all()\n",
    "\n",
    "    return [{\n",
    "        attr_name: getattr(c, attr_name) if getattr(c, attr_name) is not None else '',\n",
    "        'class': c.classification_id\n",
    "        } for c in citations]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up two tokenisers for the `subject` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import RegexpParser\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "def tokenise_subjects_simple(subject):\n",
    "    if subject == '' or subject is None:\n",
    "        return []\n",
    "    return list(set([s.strip().lower() for s in subject.split(',')]))\n",
    "\n",
    "\n",
    "def tokenise_subjects_phrases(subject):\n",
    "    if subject == '' or subject is None:\n",
    "        return []\n",
    "    split_subjects = []\n",
    "    phrase_pattern = 'CHUNK:{<JJ>*<NN.?>*<VBG>*}'\n",
    "    phrase_chunker = RegexpParser(phrase_pattern)\n",
    "    for s in subject.split(','):\n",
    "        tokens = word_tokenize(s.strip().lower())\n",
    "        tags = pos_tag(tokens)\n",
    "        phrases = [' '.join([leaf[0] for leaf in c.leaves()]) for c in phrase_chunker.parse(tags) if hasattr(c, 'label') and c.label() == 'CHUNK']\n",
    "        for phrase in phrases:\n",
    "            phrase_tokens = word_tokenize(phrase)\n",
    "            phrase_tags = pos_tag(phrase_tokens)\n",
    "            lemmatised_phrase = []\n",
    "            for pto, pta in phrase_tags:\n",
    "                wn_tag = {\n",
    "                    'n': wn.NOUN,\n",
    "                    'j': wn.ADJ,\n",
    "                    'v': wn.VERB,\n",
    "                    'r': wn.ADV\n",
    "                    }.get(pta[0].lower(), None)\n",
    "                if wn_tag is None:\n",
    "                    continue\n",
    "                lemmatised = WordNetLemmatizer().lemmatize(pto, wn_tag)\n",
    "                lemmatised_phrase.append(lemmatised)\n",
    "            if len(lemmatised_phrase) > 0:\n",
    "                lemmatised_phrase = ' '.join(lemmatised_phrase)\n",
    "                split_subjects.append(lemmatised_phrase)\n",
    "    return list(set(split_subjects))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple tokeniser just splits the string by commas (and converts to lowercase). The tags should be fairly standardised, so this shouldn't be too much of a problem.\n",
    "\n",
    "Just to see if it's more accurate/helpful to process the tags in a different way, we've also defined the 'phrase' tokeniser, which goes through several steps:\n",
    "1. Split the whole string by commas (assumes the field is a comma-separated list of phrasal tags);\n",
    "2. Tokenise the phrasal subject tags and identify parts of speech;\n",
    "3. Extract sub-phrases based on those parts of speech (i.e. adjective-noun-verb combinations, ignoring conjunctives like *and*);\n",
    "4. Lemmatise the words in those phrases to reduce differences between related tags (e.g. science and sciences both become *science*);\n",
    "5. Join the words back together to form a lemmatised sub-phrase.\n",
    "\n",
    "The two tokenisers will generate different length lists from the same input string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ecology', 'aquatic science', 'nature and landscape conservation']\n",
      "['nature', 'ecology', 'aquatic science', 'landscape conservation']\n",
      "['behavior and systematics', 'evolution', 'ecology', 'nature and landscape conservation']\n",
      "['systematics', 'behavior', 'evolution', 'nature', 'landscape conservation', 'ecology']\n"
     ]
    }
   ],
   "source": [
    "print(tokenise_subjects_simple('Ecology,Aquatic Science,Nature and Landscape Conservation'))\n",
    "print(tokenise_subjects_phrases('Ecology,Aquatic Science,Nature and Landscape Conservation'))\n",
    "\n",
    "print(tokenise_subjects_simple('Ecology,Ecology, Evolution, Behavior and Systematics,Nature and Landscape Conservation'))\n",
    "print(tokenise_subjects_phrases('Ecology,Ecology, Evolution, Behavior and Systematics,Nature and Landscape Conservation'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the list of records for training + testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "records = get_data('subject')\n",
    "records_df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both tokenisation processes will otherwise follow the same transformation pipeline in order to extract the train/test data, so we can define a method that takes the tokeniser as an argument to avoid repeating ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def transform_data(tokeniser):\n",
    "    \n",
    "    # a list of all possible tags\n",
    "    all_items = list(set([item for r in records for item in tokeniser(r['subject'])]))\n",
    "    print(len(all_items))\n",
    "    \n",
    "    # define a binariser and fit it to the list of tags\n",
    "    binariser = MultiLabelBinarizer()\n",
    "    binariser.fit([all_items])\n",
    "    \n",
    "    # transform all the subject tag data\n",
    "    all_x = binariser.transform([tokeniser(s) for s in records_df['subject']])\n",
    "    print(records_df['subject'].to_numpy()[1])\n",
    "    print(all_x[1])\n",
    "    \n",
    "    # split into training and test data\n",
    "    train_x, test_x, train_y, test_y = train_test_split(all_x, records_df['class'],\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=123, stratify=records_df['class'])\n",
    "    \n",
    "    return {\n",
    "               'x': train_x,\n",
    "               'y': train_y\n",
    "           }, {\n",
    "               'x': test_x,\n",
    "               'y': test_y\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a list of all the possible tags so that we can binarise them (convert them to a numeric representation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n",
      "Ecology,Aquatic Science,Nature and Landscape Conservation\n",
      "[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "train_simple, test_simple = transform_data(tokenise_subjects_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n",
      "Ecology,Aquatic Science,Nature and Landscape Conservation\n",
      "[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "train_phrases, test_phrases = transform_data(tokenise_subjects_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the KMeans cluster models. Again, they'll follow the same pipeline, so it's easier to define a generic method that takes the train/test data as an argument.\n",
    "\n",
    "Since this is *unsupervised* learning, it cannot be scored for accuracy until it's combined with the *supervised* methods. We may need to generate several models with different settings (e.g. for `n_clusters`) to fine-tune the model at a later point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "def create_model(train, n_clusters=2):\n",
    "    model = KMeans(n_clusters=n_clusters)\n",
    "    model.fit(train['x'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_simple = create_model(train_simple)\n",
    "model_phrases = create_model(train_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the model can be saved for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'kmeans_model_subject.pk'\n",
    "with open(filename, 'wb') as f:\n",
    "    dill.dump(model_simple, f)\n",
    "    \n",
    "\n",
    "# then to load again:\n",
    "with open(filename, 'rb') as f:\n",
    "    loaded_model = dill.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
